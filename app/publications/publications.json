[
    {
        "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
        "pdf": "http://arxiv.org/abs/2409.18025v3",
        "author": [
            "Jakub \u0141ucki",
            "Boyi Wei",
            "Yangsibo Huang",
            "Peter Henderson",
            "Florian Tram\u00e8r",
            "Javier Rando"
        ],
        "abstract": "Large language models are finetuned to refuse questions about hazardous\nknowledge, but these protections can often be bypassed. Unlearning methods aim\nat completely removing hazardous capabilities from models and make them\ninaccessible to adversaries. This work challenges the fundamental differences\nbetween unlearning and traditional safety post-training from an adversarial\nperspective. We demonstrate that existing jailbreak methods, previously\nreported as ineffective against unlearning, can be successful when applied\ncarefully. Furthermore, we develop a variety of adaptive methods that recover\nmost supposedly unlearned capabilities. For instance, we show that finetuning\non 10 unrelated examples or removing specific directions in the activation\nspace can recover most hazardous capabilities for models edited with RMU, a\nstate-of-the-art unlearning method. Our findings challenge the robustness of\ncurrent unlearning approaches and question their advantages over safety\ntraining.",
        "published": "2024-09-26T16:32:19Z",
        "updated": "2024-11-08T22:06:41Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "Spotlight paper at Neurips 2024 SoLaR workshop"
    },
    {
        "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
        "pdf": "http://arxiv.org/abs/2406.14526v1",
        "author": [
            "Luxi He",
            "Yangsibo Huang",
            "Weijia Shi",
            "Tinghao Xie",
            "Haotian Liu",
            "Yue Wang",
            "Luke Zettlemoyer",
            "Chiyuan Zhang",
            "Danqi Chen",
            "Peter Henderson"
        ],
        "abstract": "Recent studies show that image and video generation models can be prompted to\nreproduce copyrighted content from their training data, raising serious legal\nconcerns around copyright infringement. Copyrighted characters, in particular,\npose a difficult challenge for image generation services, with at least one\nlawsuit already awarding damages based on the generation of these characters.\nYet, little research has empirically examined this issue. We conduct a\nsystematic evaluation to fill this gap. First, we build CopyCat, an evaluation\nsuite consisting of diverse copyrighted characters and a novel evaluation\npipeline. Our evaluation considers both the detection of similarity to\ncopyrighted characters and generated image's consistency with user input. Our\nevaluation systematically shows that both image and video generation models can\nstill generate characters even if characters' names are not explicitly\nmentioned in the prompt, sometimes with only two generic keywords (e.g.,\nprompting with \"videogame, plumber\" consistently generates Nintendo's Mario\ncharacter). We then introduce techniques to semi-automatically identify such\nkeywords or descriptions that trigger character generation. Using our\nevaluation suite, we study runtime mitigation strategies, including both\nexisting methods and new strategies we propose. Our findings reveal that\ncommonly employed strategies, such as prompt rewriting in the DALL-E system,\nare not sufficient as standalone guardrails. These strategies must be coupled\nwith other approaches, like negative prompting, to effectively reduce the\nunintended generation of copyrighted characters. Our work provides empirical\ngrounding to the discussion of copyright mitigation strategies and offers\nactionable insights for model deployers actively implementing them.",
        "published": "2024-06-20T17:38:16Z",
        "updated": "2024-06-20T17:38:16Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": ""
    },
    {
        "title": "Evaluating Copyright Takedown Methods for Language Models",
        "pdf": "http://arxiv.org/abs/2406.18664v4",
        "author": [
            "Boyi Wei",
            "Weijia Shi",
            "Yangsibo Huang",
            "Noah A. Smith",
            "Chiyuan Zhang",
            "Luke Zettlemoyer",
            "Kai Li",
            "Peter Henderson"
        ],
        "abstract": "Language models (LMs) derive their capabilities from extensive training on\ndiverse data, including potentially copyrighted material. These models can\nmemorize and generate content similar to their training data, posing potential\nconcerns. Therefore, model creators are motivated to develop mitigation methods\nthat prevent generating protected content. We term this procedure as copyright\ntakedowns for LMs, noting the conceptual similarity to (but legal distinction\nfrom) the DMCA takedown This paper introduces the first evaluation of the\nfeasibility and side effects of copyright takedowns for LMs. We propose\nCoTaEval, an evaluation framework to assess the effectiveness of copyright\ntakedown methods, the impact on the model's ability to retain uncopyrightable\nfactual knowledge from the training data whose recitation is embargoed, and how\nwell the model maintains its general utility and efficiency. We examine several\nstrategies, including adding system prompts, decoding-time filtering\ninterventions, and unlearning approaches. Our findings indicate that no tested\nmethod excels across all metrics, showing significant room for research in this\nunique problem setting and indicating potential unresolved challenges for live\npolicy proposals.",
        "published": "2024-06-26T18:09:46Z",
        "updated": "2024-10-11T17:42:43Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "31 pages, 9 figures, 14 tables"
    },
    {
        "title": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety",
        "pdf": "http://arxiv.org/abs/2404.01099v2",
        "author": [
            "Luxi He",
            "Mengzhou Xia",
            "Peter Henderson"
        ],
        "abstract": "Current Large Language Models (LLMs), even those tuned for safety and\nalignment, are susceptible to jailbreaking. Some have found that just further\nfine-tuning an aligned model with benign data (i.e., data without harmful\ncontent) surprisingly leads to substantial degradation in safety. We delve into\nthe data-centric aspects of why benign fine-tuning inadvertently contributes to\njailbreaking. First, we represent fine-tuning data through two lenses:\nrepresentation and gradient spaces. Additionally, we propose a bi-directional\nanchoring method that, during the selection process, prioritizes data points\nthat are close to harmful examples and far from benign ones. Our approach\neffectively identifies subsets of benign data that are more likely to degrade\nthe model's safety after fine-tuning. Training on just 100 of these seemingly\nbenign datapoints surprisingly leads to the fine-tuned model affirmatively\nresponding to >70% of tested harmful requests, compared to <20% after\nfine-tuning on randomly selected data. We also observe that the selected data\nfrequently appear as lists, bullet points, or math questions, indicating a\nsystematic pattern in fine-tuning data that contributes to jailbreaking.",
        "published": "2024-04-01T13:12:30Z",
        "updated": "2024-08-20T17:54:08Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": ""
    },
    {
        "title": "AI Risk Management Should Incorporate Both Safety and Security",
        "pdf": "http://arxiv.org/abs/2405.19524v1",
        "author": [
            "Xiangyu Qi",
            "Yangsibo Huang",
            "Yi Zeng",
            "Edoardo Debenedetti",
            "Jonas Geiping",
            "Luxi He",
            "Kaixuan Huang",
            "Udari Madhushani",
            "Vikash Sehwag",
            "Weijia Shi",
            "Boyi Wei",
            "Tinghao Xie",
            "Danqi Chen",
            "Pin-Yu Chen",
            "Jeffrey Ding",
            "Ruoxi Jia",
            "Jiaqi Ma",
            "Arvind Narayanan",
            "Weijie J Su",
            "Mengdi Wang",
            "Chaowei Xiao",
            "Bo Li",
            "Dawn Song",
            "Peter Henderson",
            "Prateek Mittal"
        ],
        "abstract": "The exposure of security vulnerabilities in safety-aligned language models,\ne.g., susceptibility to adversarial attacks, has shed light on the intricate\ninterplay between AI safety and AI security. Although the two disciplines now\ncome together under the overarching goal of AI risk management, they have\nhistorically evolved separately, giving rise to differing perspectives.\nTherefore, in this paper, we advocate that stakeholders in AI risk management\nshould be aware of the nuances, synergies, and interplay between safety and\nsecurity, and unambiguously take into account the perspectives of both\ndisciplines in order to devise mostly effective and holistic risk mitigation\napproaches. Unfortunately, this vision is often obfuscated, as the definitions\nof the basic concepts of \"safety\" and \"security\" themselves are often\ninconsistent and lack consensus across communities. With AI risk management\nbeing increasingly cross-disciplinary, this issue is particularly salient. In\nlight of this conceptual challenge, we introduce a unified reference framework\nto clarify the differences and interplay between AI safety and AI security,\naiming to facilitate a shared understanding and effective collaboration across\ncommunities.",
        "published": "2024-05-29T21:00:47Z",
        "updated": "2024-05-29T21:00:47Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": ""
    },
    {
        "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank\n  Modifications",
        "pdf": "http://arxiv.org/abs/2402.05162v4",
        "author": [
            "Boyi Wei",
            "Kaixuan Huang",
            "Yangsibo Huang",
            "Tinghao Xie",
            "Xiangyu Qi",
            "Mengzhou Xia",
            "Prateek Mittal",
            "Mengdi Wang",
            "Peter Henderson"
        ],
        "abstract": "Large language models (LLMs) show inherent brittleness in their safety\nmechanisms, as evidenced by their susceptibility to jailbreaking and even\nnon-malicious fine-tuning. This study explores this brittleness of safety\nalignment by leveraging pruning and low-rank modifications. We develop methods\nto identify critical regions that are vital for safety guardrails, and that are\ndisentangled from utility-relevant regions at both the neuron and rank levels.\nSurprisingly, the isolated regions we find are sparse, comprising about $3\\%$\nat the parameter level and $2.5\\%$ at the rank level. Removing these regions\ncompromises safety without significantly impacting utility, corroborating the\ninherent brittleness of the model's safety mechanisms. Moreover, we show that\nLLMs remain vulnerable to low-cost fine-tuning attacks even when modifications\nto the safety-critical regions are restricted. These findings underscore the\nurgent need for more robust safety strategies in LLMs.",
        "published": "2024-02-07T18:34:38Z",
        "updated": "2024-10-24T19:21:52Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "22 pages, 9 figures. Project page is available at\n  https://boyiwei.com/alignment-attribution/"
    },
    {
        "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
        "pdf": "http://arxiv.org/abs/2306.13213v2",
        "author": [
            "Xiangyu Qi",
            "Kaixuan Huang",
            "Ashwinee Panda",
            "Peter Henderson",
            "Mengdi Wang",
            "Prateek Mittal"
        ],
        "abstract": "Recently, there has been a surge of interest in integrating vision into Large\nLanguage Models (LLMs), exemplified by Visual Language Models (VLMs) such as\nFlamingo and GPT-4. This paper sheds light on the security and safety\nimplications of this trend. First, we underscore that the continuous and\nhigh-dimensional nature of the visual input makes it a weak link against\nadversarial attacks, representing an expanded attack surface of\nvision-integrated LLMs. Second, we highlight that the versatility of LLMs also\npresents visual attackers with a wider array of achievable adversarial\nobjectives, extending the implications of security failures beyond mere\nmisclassification. As an illustration, we present a case study in which we\nexploit visual adversarial examples to circumvent the safety guardrail of\naligned LLMs with integrated vision. Intriguingly, we discover that a single\nvisual adversarial example can universally jailbreak an aligned LLM, compelling\nit to heed a wide range of harmful instructions that it otherwise would not)\nand generate harmful content that transcends the narrow scope of a `few-shot'\nderogatory corpus initially employed to optimize the adversarial example. Our\nstudy underscores the escalating adversarial risks associated with the pursuit\nof multimodality. Our findings also connect the long-studied adversarial\nvulnerabilities of neural networks to the nascent field of AI alignment. The\npresented attack suggests a fundamental adversarial challenge for AI alignment,\nespecially in light of the emerging trend toward multimodality in frontier\nfoundation models.",
        "published": "2023-06-22T22:13:03Z",
        "updated": "2023-08-16T22:38:55Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": ""
    },
    {
        "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety\n  Refusal Behaviors",
        "pdf": "http://arxiv.org/abs/2406.14598v1",
        "author": [
            "Tinghao Xie",
            "Xiangyu Qi",
            "Yi Zeng",
            "Yangsibo Huang",
            "Udari Madhushani Sehwag",
            "Kaixuan Huang",
            "Luxi He",
            "Boyi Wei",
            "Dacheng Li",
            "Ying Sheng",
            "Ruoxi Jia",
            "Bo Li",
            "Kai Li",
            "Danqi Chen",
            "Peter Henderson",
            "Prateek Mittal"
        ],
        "abstract": "Evaluating aligned large language models' (LLMs) ability to recognize and\nreject unsafe user requests is crucial for safe, policy-compliant deployments.\nExisting evaluation efforts, however, face three limitations that we address\nwith SORRY-Bench, our proposed benchmark. First, existing methods often use\ncoarse-grained taxonomies of unsafe topics, and are over-representing some\nfine-grained topics. For example, among the ten existing datasets that we\nevaluated, tests for refusals of self-harm instructions are over 3x less\nrepresented than tests for fraudulent activities. SORRY-Bench improves on this\nby using a fine-grained taxonomy of 45 potentially unsafe topics, and 450\nclass-balanced unsafe instructions, compiled through human-in-the-loop methods.\nSecond, linguistic characteristics and formatting of prompts are often\noverlooked, like different languages, dialects, and more -- which are only\nimplicitly considered in many evaluations. We supplement SORRY-Bench with 20\ndiverse linguistic augmentations to systematically examine these effects.\nThird, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation,\nwhich can be computationally expensive. We investigate design choices for\ncreating a fast, accurate automated safety evaluator. By collecting 7K+ human\nannotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs,\nwe show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale\nLLMs, with lower computational cost. Putting these together, we evaluate over\n40 proprietary and open-source LLMs on SORRY-Bench, analyzing their distinctive\nrefusal behaviors. We hope our effort provides a building block for systematic\nevaluations of LLMs' safety refusal capabilities, in a balanced, granular, and\nefficient manner.",
        "published": "2024-06-20T17:56:07Z",
        "updated": "2024-06-20T17:56:07Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": ""
    },
    {
        "title": "Safety Alignment Should Be Made More Than Just a Few Tokens Deep",
        "pdf": "http://arxiv.org/abs/2406.05946v1",
        "author": [
            "Xiangyu Qi",
            "Ashwinee Panda",
            "Kaifeng Lyu",
            "Xiao Ma",
            "Subhrajit Roy",
            "Ahmad Beirami",
            "Prateek Mittal",
            "Peter Henderson"
        ],
        "abstract": "The safety alignment of current Large Language Models (LLMs) is vulnerable.\nRelatively simple attacks, or even benign fine-tuning, can jailbreak aligned\nmodels. We argue that many of these vulnerabilities are related to a shared\nunderlying issue: safety alignment can take shortcuts, wherein the alignment\nadapts a model's generative distribution primarily over only its very first few\noutput tokens. We refer to this issue as shallow safety alignment. In this\npaper, we present case studies to explain why shallow safety alignment can\nexist and provide evidence that current aligned LLMs are subject to this issue.\nWe also show how these findings help explain multiple recently discovered\nvulnerabilities in LLMs, including the susceptibility to adversarial suffix\nattacks, prefilling attacks, decoding parameter attacks, and fine-tuning\nattacks. Importantly, we discuss how this consolidated notion of shallow safety\nalignment sheds light on promising research directions for mitigating these\nvulnerabilities. For instance, we show that deepening the safety alignment\nbeyond just the first few tokens can often meaningfully improve robustness\nagainst some common exploits. Finally, we design a regularized finetuning\nobjective that makes the safety alignment more persistent against fine-tuning\nattacks by constraining updates on initial tokens. Overall, we advocate that\nfuture safety alignment should be made more than just a few tokens deep.",
        "published": "2024-06-10T00:35:23Z",
        "updated": "2024-06-10T00:35:23Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": ""
    },
    {
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users\n  Do Not Intend To!",
        "pdf": "http://arxiv.org/abs/2310.03693v1",
        "author": [
            "Xiangyu Qi",
            "Yi Zeng",
            "Tinghao Xie",
            "Pin-Yu Chen",
            "Ruoxi Jia",
            "Prateek Mittal",
            "Peter Henderson"
        ],
        "abstract": "Optimizing large language models (LLMs) for downstream use cases often\ninvolves the customization of pre-trained LLMs through further fine-tuning.\nMeta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5\nTurbo on custom datasets also encourage this practice. But, what are the safety\ncosts associated with such custom fine-tuning? We note that while existing\nsafety alignment infrastructures can restrict harmful behaviors of LLMs at\ninference time, they do not cover safety risks when fine-tuning privileges are\nextended to end-users. Our red teaming studies find that the safety alignment\nof LLMs can be compromised by fine-tuning with only a few adversarially\ndesigned training examples. For instance, we jailbreak GPT-3.5 Turbo's safety\nguardrails by fine-tuning it on only 10 such examples at a cost of less than\n$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful\ninstructions. Disconcertingly, our research also reveals that, even without\nmalicious intent, simply fine-tuning with benign and commonly used datasets can\nalso inadvertently degrade the safety alignment of LLMs, though to a lesser\nextent. These findings suggest that fine-tuning aligned LLMs introduces new\nsafety risks that current safety infrastructures fall short of addressing --\neven if a model's initial safety alignment is impeccable, it is not necessarily\nto be maintained after custom fine-tuning. We outline and critically analyze\npotential mitigations and advocate for further research efforts toward\nreinforcing safety protocols for the custom fine-tuning of aligned LLMs.",
        "published": "2023-10-05T17:12:17Z",
        "updated": "2023-10-05T17:12:17Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": ""
    }
]